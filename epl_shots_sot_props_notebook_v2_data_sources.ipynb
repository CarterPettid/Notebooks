{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "799fa389",
      "metadata": {},
      "source": [
        "# EPL Shots + Shots on Target (SOT) Props — End-to-End Notebook\n",
        "\n",
        "This notebook is a **reproducible pipeline** to model **EPL player props** for **Shots** and **Shots on Target (SOT)**, produce **fair odds**, **de-vig** bookmaker prices, backtest with **walk-forward** splits, and generate a weekly **candidate table**.\n",
        "\n",
        "**No guarantee of profit.** The goal is a rigorous process that can *detect* (or falsify) an edge.\n",
        "\n",
        "---\n",
        "\n",
        "## Where the data comes from (default plan)\n",
        "\n",
        "### 1) Player minutes + shots outcomes (historical)\n",
        "**FBref via the `soccerdata` Python package**:\n",
        "- Match schedule (`read_schedule`) gives match date + home/away teams + a stable `game_id`.\n",
        "- Lineups (`read_lineup`) give **started** + **minutes_played** + position.\n",
        "- Shot events (`read_shot_events`) are aggregated to per-player **shots** and **SOT** per match.\n",
        "\n",
        "This is enough to build a player-match table for 5+ seasons **without a paid stats feed**.\n",
        "\n",
        "### 2) Bookmaker odds (weekly + optional historical)\n",
        "**The Odds API** (requires an API key):\n",
        "- Sport key: `soccer_epl`\n",
        "- Player-prop markets used here: `player_shots`, `player_shots_on_target`\n",
        "- Fetch **upcoming** odds via the event-odds endpoint, and (if you have a paid plan) fetch **historical snapshots** for backtests.\n",
        "\n",
        "If you don't want to use an API, you can also provide `odds_df.csv` / `upcoming_odds.csv` manually in the schema described below.\n",
        "\n",
        "---\n",
        "\n",
        "## Files this notebook will create or use (CSV)\n",
        "\n",
        "### `model_df.csv` (auto-built if missing)\n",
        "A player-match history table with at least:\n",
        "- `match_id, date, season, team, opponent, home_away`\n",
        "- `player_id, player_name, position`\n",
        "- `minutes, started`\n",
        "- targets: `shots, sot`\n",
        "\n",
        "### `odds_df.csv` (you provide OR build from API snapshots you store)\n",
        "Schema:\n",
        "- `match_id, date, player_id, market, line, odds_over, odds_under, book, timestamp`\n",
        "Optional:\n",
        "- `odds_over_close, odds_under_close, timestamp_close`\n",
        "\n",
        "### `upcoming_odds.csv` (optional; can be pulled from The Odds API)\n",
        "Same schema as `odds_df.csv` but for upcoming matches.\n",
        "\n",
        "---\n",
        "\n",
        "## What you get out\n",
        "- Minutes uncertainty model: **P(start)** + **minutes distribution**\n",
        "- Shots model: **Negative Binomial** rate per 90 + minutes integration\n",
        "- SOT model: **direct NB** vs **structural (Shots × Accuracy)**\n",
        "- De-vig (proportional + Shin)\n",
        "- Walk-forward backtest + calibration plots + (optional) CLV\n",
        "- `candidates.csv` + `model_version.json`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decc53b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Setup & config\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import brier_score_loss, log_loss\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.discrete.discrete_model import NegativeBinomial\n",
        "\n",
        "RNG = np.random.default_rng(7)\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "pd.set_option(\"display.width\", 200)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_dir: str = \"data\"\n",
        "    model_df_path: str = \"data/model_df.csv\"\n",
        "    odds_df_path: str = \"data/odds_df.csv\"              # historical (optional)\n",
        "    upcoming_odds_path: str = \"data/upcoming_odds.csv\"  # upcoming (optional)\n",
        "    cache_dir: str = \"cache\"\n",
        "\n",
        "    # Feature params\n",
        "    ewm_span_player: int = 10\n",
        "    ewm_span_team: int = 12\n",
        "    min_minutes_for_rates: float = 10.0\n",
        "\n",
        "    # Accuracy smoothing prior: p = (sot + a) / (shots + a + b)\n",
        "    acc_alpha: float = 2.0\n",
        "    acc_beta: float = 6.0\n",
        "\n",
        "    # Minutes model\n",
        "    minutes_max: int = 95\n",
        "    minutes_mc_samples: int = 4000\n",
        "\n",
        "    # NB sampling\n",
        "    count_mc_samples: int = 4000\n",
        "\n",
        "    # De-vig method\n",
        "    devig_method: str = \"proportional\"  # \"proportional\" or \"shin\"\n",
        "\n",
        "    # Betting / filtering rules\n",
        "    min_ev_mean: float = 0.03\n",
        "    require_ev_low_positive: bool = True\n",
        "    min_p_start: float = 0.55\n",
        "    max_minutes_ci_width: float = 35.0\n",
        "\n",
        "    # Backtest\n",
        "    walk_forward_step_days: int = 14\n",
        "    min_train_rows: int = 20000\n",
        "\n",
        "CFG = Config()\n",
        "os.makedirs(CFG.cache_dir, exist_ok=True)\n",
        "\n",
        "print(CFG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b9d929c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Utilities\n",
        "import re\nimport hashlib\n\n_TEAM_ALIASES = {\n    # common FBref vs sportsbook naming differences\n    \"man utd\": \"manchester united\",\n    \"man united\": \"manchester united\",\n    \"man city\": \"manchester city\",\n    \"spurs\": \"tottenham\",\n    \"wolves\": \"wolverhampton\",\n    \"newcastle utd\": \"newcastle united\",\n    \"brighton\": \"brighton\",\n    \"nottingham forest\": \"nottingham forest\",\n}\n\ndef canon_team(s: str) -> str:\n    s = str(s).lower()\n    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n    s = re.sub(r\"\\butd\\b\", \"united\", s)\n    s = re.sub(r\"\\bfc\\b\", \"\", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return _TEAM_ALIASES.get(s, s)\n\ndef canon_player(s: str) -> str:\n    s = str(s).lower()\n    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s\n\ndef name_hash_id(name: str, n: int = 12) -> str:\n    h = hashlib.sha1(canon_player(name).encode(\"utf-8\")).hexdigest()\n    return h[:n]\n",
        "\n",
        "def parse_date(s: pd.Series) -> pd.Series:\n",
        "    return pd.to_datetime(s, utc=False, errors=\"coerce\").dt.tz_localize(None)\n",
        "\n",
        "def assert_columns(df: pd.DataFrame, cols: List[str], name: str) -> None:\n",
        "    missing = [c for c in cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{name} missing columns: {missing}\")\n",
        "\n",
        "def safe_div(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "    b = np.asarray(b)\n",
        "    out = np.zeros_like(np.asarray(a), dtype=float)\n",
        "    m = b != 0\n",
        "    out[m] = np.asarray(a)[m] / b[m]\n",
        "    return out\n",
        "\n",
        "def pos_bucket(pos: str) -> str:\n",
        "    # Simple mapping; tweak for your data's position codes\n",
        "    p = str(pos).upper()\n",
        "    if any(x in p for x in [\"FW\", \"ST\", \"CF\"]):\n",
        "        return \"FWD\"\n",
        "    if any(x in p for x in [\"W\", \"LW\", \"RW\", \"AM\", \"CAM\", \"LM\", \"RM\"]):\n",
        "        return \"ATT_MID\"\n",
        "    if any(x in p for x in [\"MF\", \"CM\", \"DM\", \"CDM\"]):\n",
        "        return \"MID\"\n",
        "    if any(x in p for x in [\"DF\", \"CB\", \"FB\", \"LB\", \"RB\", \"WB\"]):\n",
        "        return \"DEF\"\n",
        "    if \"GK\" in p:\n",
        "        return \"GK\"\n",
        "    return \"OTHER\"\n",
        "\n",
        "def clip_minutes(x: np.ndarray, mx: int) -> np.ndarray:\n",
        "    return np.clip(x, 0, mx)\n",
        "\n",
        "def nb2_sample(mu: np.ndarray, alpha: float, rng: np.random.Generator) -> np.ndarray:\n",
        "    \"\"\"Sample Negative Binomial with mean mu and variance mu + alpha*mu^2 (NB2).\"\"\"\n",
        "    mu = np.asarray(mu, dtype=float)\n",
        "    if alpha <= 1e-12:\n",
        "        return rng.poisson(mu)\n",
        "    r = 1.0 / alpha\n",
        "    p = r / (r + mu)\n",
        "    return rng.negative_binomial(r, p, size=mu.shape)\n",
        "\n",
        "def event_from_count(count: np.ndarray, line: float) -> np.ndarray:\n",
        "    return (count > line).astype(int)\n",
        "\n",
        "def implied_prob_decimal(odds: float) -> float:\n",
        "    return 1.0 / odds if odds and odds > 0 else np.nan\n",
        "\n",
        "def to_implied_probs(odds_over: float, odds_under: float) -> Tuple[float, float]:\n",
        "    q_over = implied_prob_decimal(odds_over)\n",
        "    q_under = implied_prob_decimal(odds_under)\n",
        "    return q_over, q_under\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c86705",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Load / build data (FBref stats + odds inputs)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "def load_model_df(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    required = [\n",
        "        \"match_id\",\"date\",\"season\",\"team\",\"opponent\",\"home_away\",\n",
        "        \"player_id\",\"player_name\",\"position\",\n",
        "        \"minutes\",\"started\",\"shots\",\"sot\"\n",
        "    ]\n",
        "    assert_columns(df, required, \"model_df\")\n",
        "    df[\"date\"] = parse_date(df[\"date\"])\n",
        "    df = df.dropna(subset=[\"date\"]).copy()\n",
        "    df[\"home_away\"] = df[\"home_away\"].astype(str).str.upper().str[0]  # H/A\n",
        "    df[\"started\"] = df[\"started\"].astype(int)\n",
        "    for c in [\"minutes\",\"shots\",\"sot\"]:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df[\"pos_bucket\"] = df[\"position\"].map(pos_bucket)\n",
        "    df = df.sort_values([\"date\",\"match_id\",\"player_id\"]).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def load_odds_df(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    required = [\"match_id\",\"date\",\"player_id\",\"market\",\"line\",\"odds_over\",\"odds_under\",\"book\",\"timestamp\"]\n",
        "    assert_columns(df, required, \"odds_df\")\n",
        "    df[\"date\"] = parse_date(df[\"date\"])\n",
        "    df[\"market\"] = df[\"market\"].astype(str).str.lower()\n",
        "    df[\"line\"] = pd.to_numeric(df[\"line\"], errors=\"coerce\")\n",
        "    df[\"odds_over\"] = pd.to_numeric(df[\"odds_over\"], errors=\"coerce\")\n",
        "    df[\"odds_under\"] = pd.to_numeric(df[\"odds_under\"], errors=\"coerce\")\n",
        "    # optional close\n",
        "    for c in [\"odds_over_close\",\"odds_under_close\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\",\"line\",\"odds_over\",\"odds_under\"]).copy()\n",
        "    df = df.sort_values([\"date\",\"match_id\",\"player_id\",\"market\",\"line\"]).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "os.makedirs(CFG.data_dir, exist_ok=True)\n",
        "os.makedirs(CFG.cache_dir, exist_ok=True)\n",
        "\n",
        "# ---- 3.1 Build model_df from FBref (default stats source) ----\n",
        "# soccerdata uses FBref endpoints and caches locally.\n",
        "# Seasons are labeled by the *ending year* (e.g., 2021 == 2020-21 season).\n",
        "DEFAULT_SEASONS = [2021, 2022, 2023, 2024, 2025]\n",
        "\n",
        "def build_model_df_from_fbref(\n",
        "    seasons: List[int] = DEFAULT_SEASONS,\n",
        "    leagues: str = \"ENG-Premier League\",\n",
        "    output_csv: str = CFG.model_df_path,\n",
        ") -> pd.DataFrame:\n",
        "    try:\n",
        "        import soccerdata as sd\n",
        "    except ImportError as e:\n",
        "        raise ImportError(\n",
        "            \"Missing dependency 'soccerdata'. Install it with: pip install soccerdata\"\n",
        "        ) from e\n",
        "\n",
        "    fbref = sd.FBref(leagues=leagues, seasons=seasons, data_dir=CFG.cache_dir)\n",
        "\n",
        "    # Schedule gives us stable game_id (we use this as match_id)\n",
        "    sched = fbref.read_schedule().reset_index()\n",
        "    assert_columns(sched, [\"league\",\"season\",\"game\",\"date\",\"home_team\",\"away_team\",\"game_id\"], \"fbref.schedule\")\n",
        "    sched = sched.rename(columns={\"game_id\":\"match_id\"})\n",
        "    sched[\"date\"] = parse_date(sched[\"date\"])\n",
        "    sched = sched.dropna(subset=[\"date\"]).copy()\n",
        "\n",
        "    sched_small = sched[[\"league\",\"season\",\"game\",\"match_id\",\"date\",\"home_team\",\"away_team\"]].copy()\n",
        "    sched_small[\"home_team_c\"] = sched_small[\"home_team\"].map(canon_team)\n",
        "    sched_small[\"away_team_c\"] = sched_small[\"away_team\"].map(canon_team)\n",
        "\n",
        "    # Lineups: starter + minutes played + position\n",
        "    lineups = fbref.read_lineup().reset_index()\n",
        "    assert_columns(lineups, [\"league\",\"season\",\"game\",\"player\",\"team\",\"is_starter\",\"position\",\"minutes_played\"], \"fbref.lineups\")\n",
        "\n",
        "    lineups = lineups.merge(sched_small, on=[\"league\",\"season\",\"game\"], how=\"left\")\n",
        "    lineups = lineups.dropna(subset=[\"match_id\",\"date\"]).copy()\n",
        "    lineups = lineups.rename(columns={\n",
        "        \"player\":\"player_name\",\n",
        "        \"team\":\"team\",\n",
        "        \"minutes_played\":\"minutes\",\n",
        "        \"is_starter\":\"started\",\n",
        "        \"position\":\"position\",\n",
        "    })\n",
        "    lineups[\"started\"] = lineups[\"started\"].astype(int)\n",
        "    lineups[\"minutes\"] = pd.to_numeric(lineups[\"minutes\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    # Shot events: aggregate to shots and shots-on-target per player-match\n",
        "    shots = fbref.read_shot_events().reset_index()\n",
        "    assert_columns(shots, [\"league\",\"season\",\"game\",\"player\",\"team\",\"outcome\"], \"fbref.shot_events\")\n",
        "    shots = shots.merge(sched_small, on=[\"league\",\"season\",\"game\"], how=\"left\")\n",
        "    shots = shots.dropna(subset=[\"match_id\",\"date\"]).copy()\n",
        "\n",
        "    # Define SoT from shot outcomes: saved or goal are on-target in most data conventions.\n",
        "    out = shots[\"outcome\"].astype(str)\n",
        "    is_sot = out.str.contains(r\"goal|saved\", case=False, regex=True)\n",
        "\n",
        "    agg_shots = (\n",
        "        shots.groupby([\"match_id\",\"team\",\"player\"], as_index=False)\n",
        "        .size()\n",
        "        .rename(columns={\"size\":\"shots\"})\n",
        "    )\n",
        "    agg_sot = (\n",
        "        shots.loc[is_sot]\n",
        "        .groupby([\"match_id\",\"team\",\"player\"], as_index=False)\n",
        "        .size()\n",
        "        .rename(columns={\"size\":\"sot\"})\n",
        "    )\n",
        "    agg = agg_shots.merge(agg_sot, on=[\"match_id\",\"team\",\"player\"], how=\"left\").fillna({\"sot\":0})\n",
        "    agg = agg.rename(columns={\"player\":\"player_name\"})\n",
        "    agg[\"shots\"] = pd.to_numeric(agg[\"shots\"], errors=\"coerce\").fillna(0.0)\n",
        "    agg[\"sot\"] = pd.to_numeric(agg[\"sot\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    model_df = lineups.merge(agg, on=[\"match_id\",\"team\",\"player_name\"], how=\"left\").fillna({\"shots\":0,\"sot\":0})\n",
        "\n",
        "    # Add opponent and home/away\n",
        "    home_ctx = sched_small.rename(columns={\"home_team\":\"team\",\"away_team\":\"opponent\"})[[\"match_id\",\"date\",\"season\",\"team\",\"opponent\"]]\n",
        "    home_ctx[\"home_away\"] = \"H\"\n",
        "    away_ctx = sched_small.rename(columns={\"away_team\":\"team\",\"home_team\":\"opponent\"})[[\"match_id\",\"date\",\"season\",\"team\",\"opponent\"]]\n",
        "    away_ctx[\"home_away\"] = \"A\"\n",
        "    ctx = pd.concat([home_ctx, away_ctx], ignore_index=True)\n",
        "\n",
        "    model_df = model_df.merge(ctx, on=[\"match_id\",\"date\",\"season\",\"team\"], how=\"left\")\n",
        "    model_df[\"home_away\"] = model_df[\"home_away\"].astype(str).str.upper().str[0]\n",
        "    model_df[\"player_id\"] = model_df[\"player_name\"].map(lambda s: name_hash_id(s))\n",
        "    model_df[\"position\"] = model_df[\"position\"].astype(str)\n",
        "    model_df = model_df[[\n",
        "        \"match_id\",\"date\",\"season\",\"team\",\"opponent\",\"home_away\",\n",
        "        \"player_id\",\"player_name\",\"position\",\n",
        "        \"minutes\",\"started\",\"shots\",\"sot\"\n",
        "    ]].copy()\n",
        "\n",
        "    model_df.to_csv(output_csv, index=False)\n",
        "    print(f\"[OK] Wrote {output_csv} with {len(model_df):,} player-match rows\")\n",
        "    return model_df\n",
        "\n",
        "# ---- 3.2 Odds helpers (The Odds API) ----\n",
        "# This is optional: you can always supply odds_df.csv / upcoming_odds.csv manually.\n",
        "ODDS_API_KEY = os.environ.get(\"ODDS_API_KEY\", \"\").strip()\n",
        "ODDS_API_SPORT = \"soccer_epl\"\n",
        "ODDS_API_REGIONS = \"us\"  # player props coverage is mainly US books (per provider docs)\n",
        "ODDS_API_MARKETS = [\"player_shots\", \"player_shots_on_target\"]  # \"shots\" and \"SOT\"\n",
        "\n",
        "def _oddsapi_get(url: str, params: Dict[str, str]) -> dict:\n",
        "    r = requests.get(url, params=params, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def fetch_upcoming_odds_from_oddsapi(\n",
        "    api_key: str,\n",
        "    max_events: int = 30,\n",
        "    output_csv: Optional[str] = CFG.upcoming_odds_path,\n",
        ") -> pd.DataFrame:\n",
        "    # 1) List current/upcoming events (to get event IDs)\n",
        "    events_url = f\"https://api.the-odds-api.com/v4/sports/{ODDS_API_SPORT}/events\"\n",
        "    events = _oddsapi_get(events_url, {\"apiKey\": api_key})\n",
        "    events = events[:max_events]\n",
        "\n",
        "    # Pull FBref schedule (future matches) to map odds events -> fbref match_id (game_id)\n",
        "    if os.path.exists(CFG.model_df_path):\n",
        "        # Build a schedule table from model_df (past only). Better is to pull fresh schedule:\n",
        "        # We'll just pull schedule from FBref quickly here (cheap) if soccerdata is installed.\n",
        "        try:\n",
        "            import soccerdata as sd\n",
        "            fbref = sd.FBref(leagues=\"ENG-Premier League\", seasons=DEFAULT_SEASONS, data_dir=CFG.cache_dir)\n",
        "            sched = fbref.read_schedule().reset_index().rename(columns={\"game_id\":\"match_id\"})\n",
        "            sched[\"date\"] = parse_date(sched[\"date\"])\n",
        "            sched = sched.dropna(subset=[\"date\"]).copy()\n",
        "        except Exception:\n",
        "            sched = pd.DataFrame(columns=[\"match_id\",\"date\",\"home_team\",\"away_team\"])\n",
        "    else:\n",
        "        sched = pd.DataFrame(columns=[\"match_id\",\"date\",\"home_team\",\"away_team\"])\n",
        "\n",
        "    if len(sched):\n",
        "        sched[\"home_c\"] = sched[\"home_team\"].map(canon_team)\n",
        "        sched[\"away_c\"] = sched[\"away_team\"].map(canon_team)\n",
        "\n",
        "    rows = []\n",
        "    for ev in events:\n",
        "        event_id = ev.get(\"id\")\n",
        "        commence_time = pd.to_datetime(ev.get(\"commence_time\"), utc=True, errors=\"coerce\").tz_convert(None)\n",
        "        home = ev.get(\"home_team\")\n",
        "        away = ev.get(\"away_team\")\n",
        "        if not event_id or pd.isna(commence_time) or not home or not away:\n",
        "            continue\n",
        "\n",
        "        # 2) Query event-odds for player prop markets (shots, SOT)\n",
        "        odds_url = f\"https://api.the-odds-api.com/v4/sports/{ODDS_API_SPORT}/events/{event_id}/odds/\"\n",
        "        params = {\n",
        "            \"apiKey\": api_key,\n",
        "            \"regions\": ODDS_API_REGIONS,\n",
        "            \"markets\": \",\".join(ODDS_API_MARKETS),\n",
        "            \"oddsFormat\": \"decimal\",\n",
        "        }\n",
        "        try:\n",
        "            ev_odds = _oddsapi_get(odds_url, params)\n",
        "        except Exception as e:\n",
        "            print(\"Failed odds fetch for\", event_id, \":\", e)\n",
        "            continue\n",
        "\n",
        "        # Map to FBref match_id (game_id) if possible\n",
        "        fbref_match_id = None\n",
        "        if len(sched):\n",
        "            home_c = canon_team(home)\n",
        "            away_c = canon_team(away)\n",
        "            d0 = commence_time.date()\n",
        "            cand = sched[\n",
        "                (sched[\"home_c\"] == home_c) &\n",
        "                (sched[\"away_c\"] == away_c) &\n",
        "                (sched[\"date\"].dt.date == d0)\n",
        "            ]\n",
        "            if len(cand) == 0:\n",
        "                # allow +/- 1 day for timezone differences\n",
        "                cand = sched[\n",
        "                    (sched[\"home_c\"] == home_c) &\n",
        "                    (sched[\"away_c\"] == away_c) &\n",
        "                    (sched[\"date\"].dt.date.isin([ (commence_time - pd.Timedelta(days=1)).date(), (commence_time + pd.Timedelta(days=1)).date() ]))\n",
        "                ]\n",
        "            if len(cand):\n",
        "                fbref_match_id = cand.iloc[0][\"match_id\"]\n",
        "\n",
        "        # Parse outcomes into rows\n",
        "        for bk in ev_odds.get(\"bookmakers\", []):\n",
        "            book = bk.get(\"title\") or bk.get(\"key\") or \"unknown\"\n",
        "            ts = bk.get(\"last_update\") or ev_odds.get(\"commence_time\") or None\n",
        "            for m in bk.get(\"markets\", []):\n",
        "                mkey = m.get(\"key\")\n",
        "                if mkey not in ODDS_API_MARKETS:\n",
        "                    continue\n",
        "                market = \"shots\" if mkey == \"player_shots\" else \"sot\"\n",
        "                # outcomes are like: {\"name\":\"Over\"/\"Under\",\"description\":\"Player\",\"price\":..,\"point\":..}\n",
        "                tmp = []\n",
        "                for o in m.get(\"outcomes\", []):\n",
        "                    side = str(o.get(\"name\",\"\")).lower()\n",
        "                    player = o.get(\"description\") or o.get(\"player\") or None\n",
        "                    point = o.get(\"point\")\n",
        "                    price = o.get(\"price\")\n",
        "                    if player is None or point is None or price is None:\n",
        "                        continue\n",
        "                    tmp.append((player, float(point), side, float(price)))\n",
        "\n",
        "                if not tmp:\n",
        "                    continue\n",
        "\n",
        "                # pivot to over/under\n",
        "                dfm = pd.DataFrame(tmp, columns=[\"player_name\",\"line\",\"side\",\"price\"])\n",
        "                pivot = dfm.pivot_table(index=[\"player_name\",\"line\"], columns=\"side\", values=\"price\", aggfunc=\"first\").reset_index()\n",
        "                if \"over\" not in pivot.columns or \"under\" not in pivot.columns:\n",
        "                    continue\n",
        "\n",
        "                for _, r in pivot.iterrows():\n",
        "                    rows.append({\n",
        "                        \"match_id\": fbref_match_id or event_id,  # fallback to Odds API id if mapping fails\n",
        "                        \"date\": commence_time.date().isoformat(),\n",
        "                        \"player_id\": name_hash_id(r[\"player_name\"]),\n",
        "                        \"market\": market,\n",
        "                        \"line\": float(r[\"line\"]),\n",
        "                        \"odds_over\": float(r[\"over\"]),\n",
        "                        \"odds_under\": float(r[\"under\"]),\n",
        "                        \"book\": str(book),\n",
        "                        \"timestamp\": str(ts) if ts is not None else \"\",\n",
        "                    })\n",
        "\n",
        "        # basic rate-limit friendliness\n",
        "        time.sleep(0.25)\n",
        "\n",
        "    odds_df = pd.DataFrame(rows)\n",
        "    if output_csv is not None:\n",
        "        odds_df.to_csv(output_csv, index=False)\n",
        "        print(f\"[OK] Wrote {output_csv} with {len(odds_df):,} prop prices\")\n",
        "    return odds_df\n",
        "\n",
        "# ---- 3.3 Load data objects ----\n",
        "# model_df\n",
        "if os.path.exists(CFG.model_df_path):\n",
        "    model_df = load_model_df(CFG.model_df_path)\n",
        "else:\n",
        "    print(\"No model_df.csv found. Building from FBref via soccerdata...\")\n",
        "    model_df = build_model_df_from_fbref(output_csv=CFG.model_df_path)\n",
        "\n",
        "# odds_df (historical, for backtest)\n",
        "if os.path.exists(CFG.odds_df_path):\n",
        "    odds_df = load_odds_df(CFG.odds_df_path)\n",
        "else:\n",
        "    print(\"No odds_df.csv found at\", CFG.odds_df_path, \"(backtests vs book odds will be skipped).\")\n",
        "    odds_df = pd.DataFrame(columns=[\"match_id\",\"date\",\"player_id\",\"market\",\"line\",\"odds_over\",\"odds_under\",\"book\",\"timestamp\"])\n",
        "\n",
        "# upcoming odds (optional)\n",
        "if (not os.path.exists(CFG.upcoming_odds_path)) and ODDS_API_KEY:\n",
        "    print(\"No upcoming_odds.csv found. Pulling upcoming odds from The Odds API...\")\n",
        "    _ = fetch_upcoming_odds_from_oddsapi(ODDS_API_KEY, output_csv=CFG.upcoming_odds_path)\n",
        "\n",
        "print(\"model_df:\", model_df.shape, \"odds_df:\", odds_df.shape)\n",
        "display(model_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6991114",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Clean & merge helpers\n",
        "\n",
        "def build_match_level_tables(model_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create match-level team totals and opponent-allowed metrics.\"\"\"\n",
        "    df = model_df.copy()\n",
        "    df[\"shots_team\"] = df.groupby([\"match_id\",\"team\"])[\"shots\"].transform(\"sum\")\n",
        "    df[\"sot_team\"] = df.groupby([\"match_id\",\"team\"])[\"sot\"].transform(\"sum\")\n",
        "\n",
        "    # opponent totals for allowance\n",
        "    team_totals = (\n",
        "        df.groupby([\"match_id\",\"date\",\"team\"], as_index=False)\n",
        "          .agg(shots_for=(\"shots\",\"sum\"), sot_for=(\"sot\",\"sum\"))\n",
        "    )\n",
        "    opp_totals = team_totals.rename(columns={\"team\":\"opponent\", \"shots_for\":\"shots_against\", \"sot_for\":\"sot_against\"})\n",
        "    merged = team_totals.merge(opp_totals, on=[\"match_id\",\"date\"], how=\"left\")\n",
        "    # keep rows where opponent column matches the actual opponent\n",
        "    # We'll merge later by (match_id, team, opponent)\n",
        "    return merged\n",
        "\n",
        "def add_team_opp_rolling_features(model_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = model_df.copy()\n",
        "    match_tbl = build_match_level_tables(df)\n",
        "\n",
        "    # Build team-opponent mapping using player rows\n",
        "    key = df[[\"match_id\",\"date\",\"team\",\"opponent\"]].drop_duplicates()\n",
        "    match_tbl = key.merge(match_tbl, on=[\"match_id\",\"date\",\"team\"], how=\"left\")\n",
        "    # attach opponent totals correctly\n",
        "    # for a given (match_id, date, team, opponent), opponent's shots_for = shots_against for team\n",
        "    opp_lookup = match_tbl.rename(columns={\"team\":\"opponent\"}).copy()\n",
        "    opp_lookup = opp_lookup[[\"match_id\",\"date\",\"opponent\",\"shots_for\",\"sot_for\"]].rename(\n",
        "        columns={\"shots_for\":\"shots_against\",\"sot_for\":\"sot_against\"}\n",
        "    )\n",
        "    match_tbl = match_tbl.drop(columns=[\"shots_against\",\"sot_against\"], errors=\"ignore\").merge(\n",
        "        opp_lookup, on=[\"match_id\",\"date\",\"opponent\"], how=\"left\"\n",
        "    )\n",
        "\n",
        "    # per90 at team level: assume 90 minutes; this is a crude normalization but works pre-match\n",
        "    match_tbl[\"team_shots_for_per90\"] = match_tbl[\"shots_for\"]\n",
        "    match_tbl[\"team_sot_for_per90\"] = match_tbl[\"sot_for\"]\n",
        "    match_tbl[\"opp_shots_allowed_per90\"] = match_tbl[\"shots_against\"]\n",
        "    match_tbl[\"opp_sot_allowed_per90\"] = match_tbl[\"sot_against\"]\n",
        "\n",
        "    match_tbl = match_tbl.sort_values([\"team\",\"date\",\"match_id\"])\n",
        "\n",
        "    # EWMA rolling with shift(1) to prevent leakage\n",
        "    for c, span in [\n",
        "        (\"team_shots_for_per90\", CFG.ewm_span_team),\n",
        "        (\"team_sot_for_per90\", CFG.ewm_span_team),\n",
        "        (\"opp_shots_allowed_per90\", CFG.ewm_span_team),\n",
        "        (\"opp_sot_allowed_per90\", CFG.ewm_span_team),\n",
        "    ]:\n",
        "        match_tbl[c + \"_ewm\"] = (\n",
        "            match_tbl.groupby(\"team\")[c]\n",
        "            .transform(lambda s: s.ewm(span=span, adjust=False).mean().shift(1))\n",
        "        )\n",
        "\n",
        "    # Merge back to player rows\n",
        "    feat_cols = [c + \"_ewm\" for c in [\"team_shots_for_per90\",\"team_sot_for_per90\",\"opp_shots_allowed_per90\",\"opp_sot_allowed_per90\"]]\n",
        "    df = df.merge(match_tbl[[\"match_id\",\"date\",\"team\",\"opponent\"] + feat_cols], on=[\"match_id\",\"date\",\"team\",\"opponent\"], how=\"left\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_player_rolling_features(model_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = model_df.copy()\n",
        "    minutes = df[\"minutes\"].to_numpy()\n",
        "    denom90 = np.maximum(minutes / 90.0, CFG.min_minutes_for_rates / 90.0)\n",
        "\n",
        "    df[\"shots_per90\"] = df[\"shots\"] / denom90\n",
        "    df[\"sot_per90\"] = df[\"sot\"] / denom90\n",
        "\n",
        "    df = df.sort_values([\"player_id\",\"date\",\"match_id\"]).reset_index(drop=True)\n",
        "\n",
        "    # EWMA for rates (shifted)\n",
        "    df[\"shots_per90_ewm\"] = df.groupby(\"player_id\")[\"shots_per90\"].transform(\n",
        "        lambda s: s.ewm(span=CFG.ewm_span_player, adjust=False).mean().shift(1)\n",
        "    )\n",
        "    df[\"sot_per90_ewm\"] = df.groupby(\"player_id\")[\"sot_per90\"].transform(\n",
        "        lambda s: s.ewm(span=CFG.ewm_span_player, adjust=False).mean().shift(1)\n",
        "    )\n",
        "\n",
        "    # Smoothed historical accuracy prior (shifted cumulative)\n",
        "    g = df.groupby(\"player_id\")\n",
        "    cum_shots = g[\"shots\"].cumsum().shift(1).fillna(0.0)\n",
        "    cum_sot = g[\"sot\"].cumsum().shift(1).fillna(0.0)\n",
        "    a, b = CFG.acc_alpha, CFG.acc_beta\n",
        "    df[\"acc_prior\"] = (cum_sot + a) / (cum_shots + a + b)\n",
        "\n",
        "    # Recent starts/minutes features for minutes model\n",
        "    df[\"starts_ewm\"] = g[\"started\"].transform(lambda s: s.ewm(span=CFG.ewm_span_player, adjust=False).mean().shift(1))\n",
        "    df[\"minutes_ewm\"] = g[\"minutes\"].transform(lambda s: s.ewm(span=CFG.ewm_span_player, adjust=False).mean().shift(1))\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_rest_days(model_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = model_df.copy()\n",
        "    # Team rest days: days since last team match\n",
        "    team_dates = df[[\"team\",\"date\"]].drop_duplicates().sort_values([\"team\",\"date\"])\n",
        "    team_dates[\"team_rest_days\"] = team_dates.groupby(\"team\")[\"date\"].diff().dt.days\n",
        "    df = df.merge(team_dates, on=[\"team\",\"date\"], how=\"left\")\n",
        "\n",
        "    # Player rest days\n",
        "    player_dates = df[[\"player_id\",\"date\"]].drop_duplicates().sort_values([\"player_id\",\"date\"])\n",
        "    player_dates[\"player_rest_days\"] = player_dates.groupby(\"player_id\")[\"date\"].diff().dt.days\n",
        "    df = df.merge(player_dates, on=[\"player_id\",\"date\"], how=\"left\")\n",
        "\n",
        "    for c in [\"team_rest_days\",\"player_rest_days\"]:\n",
        "        df[c] = df[c].fillna(df[c].median())\n",
        "    return df\n",
        "\n",
        "def build_features(model_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = model_df.copy()\n",
        "    df = add_team_opp_rolling_features(df)\n",
        "    df = add_player_rolling_features(df)\n",
        "    df = add_rest_days(df)\n",
        "\n",
        "    # Home indicator\n",
        "    df[\"is_home\"] = (df[\"home_away\"] == \"H\").astype(int)\n",
        "\n",
        "    # Basic missing handling\n",
        "    for c in [\n",
        "        \"shots_per90_ewm\",\"sot_per90_ewm\",\"acc_prior\",\"starts_ewm\",\"minutes_ewm\",\n",
        "        \"team_shots_for_per90_ewm\",\"team_sot_for_per90_ewm\",\"opp_shots_allowed_per90_ewm\",\"opp_sot_allowed_per90_ewm\",\n",
        "        \"team_rest_days\",\"player_rest_days\"\n",
        "    ]:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].fillna(df[c].median())\n",
        "    return df\n",
        "\n",
        "feat_df = build_features(model_df)\n",
        "feat_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c40fa31",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) EDA (quick sanity checks)\n",
        "\n",
        "def quick_eda(df: pd.DataFrame) -> None:\n",
        "    print(\"Rows:\", df.shape[0])\n",
        "    print(\"Date range:\", df[\"date\"].min(), \"->\", df[\"date\"].max())\n",
        "    print(\"Players:\", df[\"player_id\"].nunique(), \"Teams:\", df[\"team\"].nunique())\n",
        "    print(\"\\nShots summary:\")\n",
        "    print(df[\"shots\"].describe())\n",
        "    print(\"\\nSOT summary:\")\n",
        "    print(df[\"sot\"].describe())\n",
        "    print(\"\\nMinutes summary:\")\n",
        "    print(df[\"minutes\"].describe())\n",
        "\n",
        "quick_eda(feat_df)\n",
        "\n",
        "plt.figure()\n",
        "feat_df[\"shots\"].hist(bins=30)\n",
        "plt.title(\"Shots distribution\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "feat_df[\"sot\"].hist(bins=20)\n",
        "plt.title(\"SOT distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524b64b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Minutes model: P(start) + minutes distributions\n",
        "\n",
        "MINUTES_FEATURES_NUM = [\n",
        "    \"starts_ewm\",\"minutes_ewm\",\"team_rest_days\",\"player_rest_days\",\n",
        "    \"team_shots_for_per90_ewm\",\"opp_shots_allowed_per90_ewm\",\n",
        "]\n",
        "MINUTES_FEATURES_CAT = [\"pos_bucket\",\"team\"]\n",
        "\n",
        "def fit_start_model(train: pd.DataFrame) -> Pipeline:\n",
        "    X = train[MINUTES_FEATURES_NUM + MINUTES_FEATURES_CAT]\n",
        "    y = train[\"started\"].astype(int)\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(), MINUTES_FEATURES_NUM),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), MINUTES_FEATURES_CAT),\n",
        "        ]\n",
        "    )\n",
        "    clf = LogisticRegression(max_iter=2000, C=0.8, solver=\"lbfgs\")\n",
        "    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
        "    pipe.fit(X, y)\n",
        "    return pipe\n",
        "\n",
        "def fit_minutes_regressor(train: pd.DataFrame, started_value: int) -> Pipeline:\n",
        "    sub = train[train[\"started\"] == started_value].copy()\n",
        "    X = sub[MINUTES_FEATURES_NUM + MINUTES_FEATURES_CAT]\n",
        "    y = sub[\"minutes\"].astype(float)\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", StandardScaler(), MINUTES_FEATURES_NUM),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), MINUTES_FEATURES_CAT),\n",
        "        ]\n",
        "    )\n",
        "    reg = HistGradientBoostingRegressor(\n",
        "        loss=\"squared_error\",\n",
        "        max_depth=5,\n",
        "        learning_rate=0.06,\n",
        "        max_iter=300,\n",
        "        random_state=7\n",
        "    )\n",
        "    pipe = Pipeline([(\"pre\", pre), (\"reg\", reg)])\n",
        "    pipe.fit(X, y)\n",
        "    return pipe\n",
        "\n",
        "def estimate_resid_std(train: pd.DataFrame, reg: Pipeline, started_value: int) -> float:\n",
        "    sub = train[train[\"started\"] == started_value].copy()\n",
        "    X = sub[MINUTES_FEATURES_NUM + MINUTES_FEATURES_CAT]\n",
        "    y = sub[\"minutes\"].astype(float).to_numpy()\n",
        "    pred = reg.predict(X)\n",
        "    resid = y - pred\n",
        "    # robust-ish\n",
        "    return float(np.nanmedian(np.abs(resid)) * 1.4826)\n",
        "\n",
        "@dataclass\n",
        "class MinutesModel:\n",
        "    start_pipe: Pipeline\n",
        "    min_start_pipe: Pipeline\n",
        "    min_bench_pipe: Pipeline\n",
        "    std_start: float\n",
        "    std_bench: float\n",
        "\n",
        "    def predict_p_start(self, rows: pd.DataFrame) -> np.ndarray:\n",
        "        X = rows[MINUTES_FEATURES_NUM + MINUTES_FEATURES_CAT]\n",
        "        return self.start_pipe.predict_proba(X)[:, 1]\n",
        "\n",
        "    def sample_minutes(self, rows: pd.DataFrame, n: int, rng: np.random.Generator) -> np.ndarray:\n",
        "        \"\"\"Return samples with shape (len(rows), n).\"\"\"\n",
        "        p = self.predict_p_start(rows)\n",
        "        X = rows[MINUTES_FEATURES_NUM + MINUTES_FEATURES_CAT]\n",
        "        mu_start = self.min_start_pipe.predict(X)\n",
        "        mu_bench = self.min_bench_pipe.predict(X)\n",
        "\n",
        "        # Sample start indicator\n",
        "        start_draw = rng.random((len(rows), n)) < p[:, None]\n",
        "\n",
        "        m = np.empty((len(rows), n), dtype=float)\n",
        "        # Truncated normal approximation (fast and practical)\n",
        "        m_start = rng.normal(mu_start[:, None], self.std_start, size=(len(rows), n))\n",
        "        m_bench = rng.normal(mu_bench[:, None], self.std_bench, size=(len(rows), n))\n",
        "\n",
        "        m = np.where(start_draw, m_start, m_bench)\n",
        "        m = clip_minutes(m, CFG.minutes_max)\n",
        "        return m\n",
        "\n",
        "def fit_minutes_model(train: pd.DataFrame) -> MinutesModel:\n",
        "    start_pipe = fit_start_model(train)\n",
        "    min_start = fit_minutes_regressor(train, started_value=1)\n",
        "    min_bench = fit_minutes_regressor(train, started_value=0) if (train[\"started\"] == 0).any() else fit_minutes_regressor(train, started_value=1)\n",
        "    std_start = estimate_resid_std(train, min_start, started_value=1)\n",
        "    std_bench = estimate_resid_std(train, min_bench, started_value=0) if (train[\"started\"] == 0).any() else std_start\n",
        "    return MinutesModel(start_pipe, min_start, min_bench, std_start, std_bench)\n",
        "\n",
        "# Example fit on all data (for later candidate generation); backtest will refit per fold\n",
        "minutes_model_all = fit_minutes_model(feat_df)\n",
        "\n",
        "# quick check on a random sample\n",
        "sample_rows = feat_df.sample(5, random_state=7)\n",
        "m_samp = minutes_model_all.sample_minutes(sample_rows, n=2000, rng=RNG)\n",
        "p_start = minutes_model_all.predict_p_start(sample_rows)\n",
        "pd.DataFrame({\n",
        "    \"player\": sample_rows[\"player_name\"].values,\n",
        "    \"p_start\": p_start,\n",
        "    \"min_mean\": m_samp.mean(axis=1),\n",
        "    \"min_p10\": np.quantile(m_samp, 0.10, axis=1),\n",
        "    \"min_p90\": np.quantile(m_samp, 0.90, axis=1),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f800438b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Shots model (Negative Binomial rate per 90)\n",
        "\n",
        "SHOTS_FEATURES_NUM = [\n",
        "    \"shots_per90_ewm\",\n",
        "    \"team_shots_for_per90_ewm\",\n",
        "    \"opp_shots_allowed_per90_ewm\",\n",
        "    \"team_rest_days\",\"player_rest_days\",\n",
        "    \"is_home\",\n",
        "]\n",
        "SHOTS_FEATURES_CAT = [\"pos_bucket\"]\n",
        "\n",
        "def make_design_matrix(df: pd.DataFrame, num_cols: List[str], cat_cols: List[str], fit_cols: Optional[List[str]]=None) -> Tuple[pd.DataFrame, List[str]]:\n",
        "    X_num = df[num_cols].astype(float).copy()\n",
        "    X_cat = pd.get_dummies(df[cat_cols].astype(str), prefix=cat_cols, dummy_na=False)\n",
        "    X = pd.concat([X_num, X_cat], axis=1)\n",
        "    X = sm.add_constant(X, has_constant=\"add\")\n",
        "    if fit_cols is None:\n",
        "        fit_cols = list(X.columns)\n",
        "    else:\n",
        "        # align columns\n",
        "        for c in fit_cols:\n",
        "            if c not in X.columns:\n",
        "                X[c] = 0.0\n",
        "        X = X[fit_cols]\n",
        "    return X, fit_cols\n",
        "\n",
        "@dataclass\n",
        "class NBModel:\n",
        "    res: any\n",
        "    cols: List[str]\n",
        "\n",
        "    @property\n",
        "    def alpha(self) -> float:\n",
        "        # statsmodels stores alpha as a parameter named 'alpha' in res.params sometimes; for discrete NB, it's res.params or res.model._dispersion?\n",
        "        # For NegativeBinomial (discrete), alpha is res.params[-1] if model includes it implicitly. We'll read res.model._dispersion if available.\n",
        "        if hasattr(self.res, \"params\") and hasattr(self.res.model, \"exog_names\"):\n",
        "            # Discrete NB stores ln(alpha)?? Actually discrete NB includes alpha separately accessible as res.model._dispersion if set.\n",
        "            if hasattr(self.res, \"model\") and hasattr(self.res.model, \"_dispersion\"):\n",
        "                return float(self.res.model._dispersion)\n",
        "        # Fallback: try res.params_alpha if present\n",
        "        if hasattr(self.res, \"params\") and \"alpha\" in getattr(self.res, \"params\", {}):\n",
        "            return float(self.res.params[\"alpha\"])\n",
        "        # Conservative fallback\n",
        "        return 0.4\n",
        "\n",
        "    def predict_mu90(self, rows: pd.DataFrame, num_cols: List[str], cat_cols: List[str]) -> np.ndarray:\n",
        "        X, _ = make_design_matrix(rows, num_cols, cat_cols, fit_cols=self.cols)\n",
        "        offset = np.zeros(len(rows))\n",
        "        mu90 = self.res.predict(X, offset=offset)\n",
        "        return np.asarray(mu90, dtype=float)\n",
        "\n",
        "def fit_nb_count_model(train: pd.DataFrame, y_col: str, num_cols: List[str], cat_cols: List[str]) -> NBModel:\n",
        "    df = train.copy()\n",
        "    # offset uses realized minutes\n",
        "    offset = np.log(np.maximum(df[\"minutes\"].astype(float).to_numpy() / 90.0, 1e-6))\n",
        "    y = df[y_col].astype(int).to_numpy()\n",
        "\n",
        "    X, cols = make_design_matrix(df, num_cols, cat_cols, fit_cols=None)\n",
        "    # Discrete NegativeBinomial (NB2)\n",
        "    mod = NegativeBinomial(y, X, offset=offset)\n",
        "    res = mod.fit(disp=False, maxiter=200)\n",
        "    # store estimated dispersion\n",
        "    try:\n",
        "        res.model._dispersion = float(res.params[-1]) if np.isfinite(res.params[-1]) else 0.4\n",
        "    except Exception:\n",
        "        res.model._dispersion = 0.4\n",
        "    return NBModel(res=res, cols=cols)\n",
        "\n",
        "shots_nb_all = fit_nb_count_model(feat_df.dropna(subset=SHOTS_FEATURES_NUM), \"shots\", SHOTS_FEATURES_NUM, SHOTS_FEATURES_CAT)\n",
        "print(shots_nb_all.res.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599be6af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) SOT models: (A) direct NB and (B) structural Shots × Accuracy\n",
        "\n",
        "SOT_FEATURES_NUM = [\n",
        "    \"sot_per90_ewm\",\n",
        "    \"team_sot_for_per90_ewm\",\n",
        "    \"opp_sot_allowed_per90_ewm\",\n",
        "    \"team_rest_days\",\"player_rest_days\",\n",
        "    \"is_home\",\n",
        "]\n",
        "SOT_FEATURES_CAT = [\"pos_bucket\"]\n",
        "\n",
        "# A) direct NB on SOT count with minutes offset\n",
        "sot_nb_all = fit_nb_count_model(\n",
        "    feat_df.dropna(subset=SOT_FEATURES_NUM),\n",
        "    \"sot\",\n",
        "    SOT_FEATURES_NUM,\n",
        "    SOT_FEATURES_CAT\n",
        ")\n",
        "print(sot_nb_all.res.summary())\n",
        "\n",
        "# B) accuracy model for Binomial(SOT | Shots)\n",
        "# We fit a Binomial GLM on per-shot accuracy with var_weights=shots.\n",
        "ACC_FEATURES_NUM = [\n",
        "    \"acc_prior\",\n",
        "    \"shots_per90_ewm\",\n",
        "    \"team_sot_for_per90_ewm\",\n",
        "    \"opp_sot_allowed_per90_ewm\",\n",
        "    \"is_home\",\n",
        "]\n",
        "ACC_FEATURES_CAT = [\"pos_bucket\"]\n",
        "\n",
        "@dataclass\n",
        "class BinomModel:\n",
        "    res: any\n",
        "    cols: List[str]\n",
        "\n",
        "    def predict_p(self, rows: pd.DataFrame, num_cols: List[str], cat_cols: List[str]) -> np.ndarray:\n",
        "        X, _ = make_design_matrix(rows, num_cols, cat_cols, fit_cols=self.cols)\n",
        "        p = np.asarray(self.res.predict(X), dtype=float)\n",
        "        return np.clip(p, 0.03, 0.75)\n",
        "\n",
        "def fit_accuracy_model(train: pd.DataFrame) -> BinomModel:\n",
        "    df = train.copy()\n",
        "    df = df[df[\"shots\"] > 0].copy()\n",
        "    y = (df[\"sot\"] / df[\"shots\"]).clip(0, 1).astype(float)\n",
        "    w = df[\"shots\"].astype(float).to_numpy()\n",
        "\n",
        "    X, cols = make_design_matrix(df, ACC_FEATURES_NUM, ACC_FEATURES_CAT, fit_cols=None)\n",
        "    mod = sm.GLM(y, X, family=sm.families.Binomial(), var_weights=w)\n",
        "    res = mod.fit()\n",
        "    return BinomModel(res=res, cols=cols)\n",
        "\n",
        "acc_model_all = fit_accuracy_model(feat_df)\n",
        "print(acc_model_all.res.summary())\n",
        "\n",
        "def predict_accuracy(rows: pd.DataFrame, acc_model: BinomModel) -> np.ndarray:\n",
        "    return acc_model.predict_p(rows, ACC_FEATURES_NUM, ACC_FEATURES_CAT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3455382b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9) Probability engine: compute P(Over line) for shots and SOT with minutes uncertainty\n",
        "\n",
        "def predict_over_probs_shots(\n",
        "    rows: pd.DataFrame,\n",
        "    lines: np.ndarray,\n",
        "    minutes_model: MinutesModel,\n",
        "    shots_model: NBModel,\n",
        "    n_mc: int,\n",
        "    rng: np.random.Generator\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Return shape (len(rows), len(lines))\"\"\"\n",
        "    mu90 = shots_model.predict_mu90(rows, SHOTS_FEATURES_NUM, SHOTS_FEATURES_CAT)  # mean shots at 90\n",
        "    m = minutes_model.sample_minutes(rows, n=n_mc, rng=rng)  # (N, n_mc)\n",
        "    mu = mu90[:, None] * (m / 90.0)\n",
        "    alpha = shots_model.alpha\n",
        "    shots_samp = nb2_sample(mu, alpha=alpha, rng=rng)\n",
        "    out = np.zeros((len(rows), len(lines)), dtype=float)\n",
        "    for j, line in enumerate(lines):\n",
        "        out[:, j] = (shots_samp > line).mean(axis=1)\n",
        "    return out\n",
        "\n",
        "def predict_over_probs_sot_direct(\n",
        "    rows: pd.DataFrame,\n",
        "    lines: np.ndarray,\n",
        "    minutes_model: MinutesModel,\n",
        "    sot_model: NBModel,\n",
        "    n_mc: int,\n",
        "    rng: np.random.Generator\n",
        ") -> np.ndarray:\n",
        "    mu90 = sot_model.predict_mu90(rows, SOT_FEATURES_NUM, SOT_FEATURES_CAT)\n",
        "    m = minutes_model.sample_minutes(rows, n=n_mc, rng=rng)\n",
        "    mu = mu90[:, None] * (m / 90.0)\n",
        "    alpha = sot_model.alpha\n",
        "    sot_samp = nb2_sample(mu, alpha=alpha, rng=rng)\n",
        "    out = np.zeros((len(rows), len(lines)), dtype=float)\n",
        "    for j, line in enumerate(lines):\n",
        "        out[:, j] = (sot_samp > line).mean(axis=1)\n",
        "    return out\n",
        "\n",
        "def predict_over_probs_sot_structural(\n",
        "    rows: pd.DataFrame,\n",
        "    lines: np.ndarray,\n",
        "    minutes_model: MinutesModel,\n",
        "    shots_model: NBModel,\n",
        "    acc_model: BinomModel,\n",
        "    n_mc: int,\n",
        "    rng: np.random.Generator\n",
        ") -> np.ndarray:\n",
        "    mu90_shots = shots_model.predict_mu90(rows, SHOTS_FEATURES_NUM, SHOTS_FEATURES_CAT)\n",
        "    p_acc = predict_accuracy(rows, acc_model)  # (N,)\n",
        "    m = minutes_model.sample_minutes(rows, n=n_mc, rng=rng)\n",
        "    mu_shots = mu90_shots[:, None] * (m / 90.0)\n",
        "    shots_samp = nb2_sample(mu_shots, alpha=shots_model.alpha, rng=rng)\n",
        "    # Binomial per draw\n",
        "    p = np.clip(p_acc[:, None], 0.01, 0.9)\n",
        "    sot_samp = rng.binomial(shots_samp, p)\n",
        "    out = np.zeros((len(rows), len(lines)), dtype=float)\n",
        "    for j, line in enumerate(lines):\n",
        "        out[:, j] = (sot_samp > line).mean(axis=1)\n",
        "    return out\n",
        "\n",
        "# Quick demo on a few rows\n",
        "demo = feat_df.sample(3, random_state=7)\n",
        "shots_lines = np.array([0.5, 1.5, 2.5])\n",
        "sot_lines = np.array([0.5, 1.5])\n",
        "\n",
        "p_shots = predict_over_probs_shots(demo, shots_lines, minutes_model_all, shots_nb_all, n_mc=2000, rng=RNG)\n",
        "p_sot_a = predict_over_probs_sot_direct(demo, sot_lines, minutes_model_all, sot_nb_all, n_mc=2000, rng=RNG)\n",
        "p_sot_b = predict_over_probs_sot_structural(demo, sot_lines, minutes_model_all, shots_nb_all, acc_model_all, n_mc=2000, rng=RNG)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"player\": demo[\"player_name\"].values,\n",
        "    \"p_shots>1.5\": p_shots[:,1],\n",
        "    \"p_sot>0.5_direct\": p_sot_a[:,0],\n",
        "    \"p_sot>0.5_struct\": p_sot_b[:,0],\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b443d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10) De-vig methods (proportional + Shin)\n",
        "\n",
        "def devig_proportional(q_over: float, q_under: float) -> Tuple[float, float]:\n",
        "    s = q_over + q_under\n",
        "    if not np.isfinite(s) or s <= 0:\n",
        "        return np.nan, np.nan\n",
        "    return q_over / s, q_under / s\n",
        "\n",
        "def devig_shin(q_over: float, q_under: float, tol: float=1e-9, max_iter: int=100) -> Tuple[float, float]:\n",
        "    \"\"\"Shin method for a two-outcome market. Returns (p_over, p_under).\"\"\"\n",
        "    qs = np.array([q_over, q_under], dtype=float)\n",
        "    if np.any(~np.isfinite(qs)) or np.any(qs <= 0):\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # Normalize to avoid numeric issues\n",
        "    qs = qs / qs.sum()\n",
        "\n",
        "    # Solve for z in [0,1) using binary search (monotonic)\n",
        "    lo, hi = 0.0, 0.999999\n",
        "    for _ in range(max_iter):\n",
        "        z = 0.5 * (lo + hi)\n",
        "        denom = 1 - z\n",
        "        ps = (np.sqrt(z*z + 4*denom*qs) - z) / (2*denom)\n",
        "        s = ps.sum()\n",
        "        if abs(s - 1.0) < tol:\n",
        "            break\n",
        "        if s > 1.0:\n",
        "            lo = z\n",
        "        else:\n",
        "            hi = z\n",
        "    ps = (np.sqrt(z*z + 4*(1-z)*qs) - z) / (2*(1-z))\n",
        "    ps = ps / ps.sum()\n",
        "    return float(ps[0]), float(ps[1])\n",
        "\n",
        "def devig_row(row: pd.Series, method: str) -> Tuple[float, float]:\n",
        "    q_over, q_under = to_implied_probs(row[\"odds_over\"], row[\"odds_under\"])\n",
        "    if method == \"proportional\":\n",
        "        return devig_proportional(q_over, q_under)\n",
        "    if method == \"shin\":\n",
        "        return devig_shin(q_over, q_under)\n",
        "    raise ValueError(f\"Unknown devig method: {method}\")\n",
        "\n",
        "def ev_decimal(p: float, odds: float) -> float:\n",
        "    # EV per 1 unit stake\n",
        "    return p * (odds - 1.0) - (1.0 - p)\n",
        "\n",
        "# demo on odds_df\n",
        "tmp = odds_df.head(5).copy()\n",
        "tmp[[\"p_mkt_over\",\"p_mkt_under\"]] = tmp.apply(lambda r: pd.Series(devig_row(r, CFG.devig_method)), axis=1)\n",
        "tmp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4631fa4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 11) Merge odds with features and label outcomes\n",
        "\n",
        "def merge_odds_with_features(odds_df: pd.DataFrame, feat_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Join on match_id, date, player_id\n",
        "    merged = odds_df.merge(\n",
        "        feat_df,\n",
        "        on=[\"match_id\",\"date\",\"player_id\"],\n",
        "        how=\"left\",\n",
        "        suffixes=(\"\", \"_feat\")\n",
        "    )\n",
        "    # keep only rows where we have outcomes (for backtesting)\n",
        "    merged = merged.dropna(subset=[\"minutes\",\"shots\",\"sot\"])\n",
        "    # outcome labels\n",
        "    merged[\"y_count\"] = np.where(merged[\"market\"] == \"shots\", merged[\"shots\"], merged[\"sot\"])\n",
        "    merged[\"y_over\"] = (merged[\"y_count\"] > merged[\"line\"]).astype(int)\n",
        "    return merged\n",
        "\n",
        "odds_feat = merge_odds_with_features(odds_df, feat_df)\n",
        "print(\"Merged odds rows:\", odds_feat.shape)\n",
        "odds_feat.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94bc4e62",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 12) Walk-forward backtest\n",
        "\n",
        "def build_prediction_block(\n",
        "    block: pd.DataFrame,\n",
        "    minutes_model: MinutesModel,\n",
        "    shots_model: NBModel,\n",
        "    sot_direct_model: NBModel,\n",
        "    acc_model: BinomModel,\n",
        "    n_mc: int,\n",
        "    rng: np.random.Generator\n",
        ") -> pd.DataFrame:\n",
        "    out = block.copy()\n",
        "    out[\"p_start\"] = minutes_model.predict_p_start(out)\n",
        "    # minutes CI\n",
        "    m_samp = minutes_model.sample_minutes(out, n=n_mc, rng=rng)\n",
        "    out[\"min_mean\"] = m_samp.mean(axis=1)\n",
        "    out[\"min_p10\"] = np.quantile(m_samp, 0.10, axis=1)\n",
        "    out[\"min_p90\"] = np.quantile(m_samp, 0.90, axis=1)\n",
        "    out[\"min_ci_width\"] = out[\"min_p90\"] - out[\"min_p10\"]\n",
        "\n",
        "    # Predict probabilities depending on market\n",
        "    out[\"p_model_over_direct\"] = np.nan\n",
        "    out[\"p_model_over_struct\"] = np.nan\n",
        "\n",
        "    # Shots\n",
        "    mask_sh = out[\"market\"] == \"shots\"\n",
        "    if mask_sh.any():\n",
        "        lines = out.loc[mask_sh, \"line\"].to_numpy()\n",
        "        # vectorize by grouping unique lines (faster)\n",
        "        uniq_lines = np.unique(lines)\n",
        "        probs = predict_over_probs_shots(out.loc[mask_sh], uniq_lines, minutes_model, shots_model, n_mc=n_mc, rng=rng)\n",
        "        line_to_idx = {l:i for i,l in enumerate(uniq_lines)}\n",
        "        out.loc[mask_sh, \"p_model_over_direct\"] = [probs[i, line_to_idx[l]] for i,l in enumerate(lines)]\n",
        "        out.loc[mask_sh, \"p_model_over_struct\"] = out.loc[mask_sh, \"p_model_over_direct\"]  # same for shots\n",
        "\n",
        "    # SOT: compute both direct and structural\n",
        "    mask_sot = out[\"market\"] == \"sot\"\n",
        "    if mask_sot.any():\n",
        "        lines = out.loc[mask_sot, \"line\"].to_numpy()\n",
        "        uniq_lines = np.unique(lines)\n",
        "\n",
        "        probs_a = predict_over_probs_sot_direct(out.loc[mask_sot], uniq_lines, minutes_model, sot_direct_model, n_mc=n_mc, rng=rng)\n",
        "        probs_b = predict_over_probs_sot_structural(out.loc[mask_sot], uniq_lines, minutes_model, shots_model, acc_model, n_mc=n_mc, rng=rng)\n",
        "\n",
        "        line_to_idx = {l:i for i,l in enumerate(uniq_lines)}\n",
        "        out.loc[mask_sot, \"p_model_over_direct\"] = [probs_a[i, line_to_idx[l]] for i,l in enumerate(lines)]\n",
        "        out.loc[mask_sot, \"p_model_over_struct\"] = [probs_b[i, line_to_idx[l]] for i,l in enumerate(lines)]\n",
        "\n",
        "    # Market probs\n",
        "    out[[\"p_mkt_over\",\"p_mkt_under\"]] = out.apply(lambda r: pd.Series(devig_row(r, CFG.devig_method)), axis=1)\n",
        "\n",
        "    # EVs using offered odds\n",
        "    out[\"ev_over_direct\"] = out.apply(lambda r: ev_decimal(r[\"p_model_over_direct\"], r[\"odds_over\"]), axis=1)\n",
        "    out[\"ev_over_struct\"] = out.apply(lambda r: ev_decimal(r[\"p_model_over_struct\"], r[\"odds_over\"]), axis=1)\n",
        "\n",
        "    # CLV if available\n",
        "    if \"odds_over_close\" in out.columns and \"odds_under_close\" in out.columns:\n",
        "        def devig_close(r):\n",
        "            if pd.isna(r.get(\"odds_over_close\")) or pd.isna(r.get(\"odds_under_close\")):\n",
        "                return np.nan\n",
        "            q_over, q_under = to_implied_probs(r[\"odds_over_close\"], r[\"odds_under_close\"])\n",
        "            p_over, _ = devig_proportional(q_over, q_under)\n",
        "            return p_over\n",
        "        out[\"p_close_over\"] = out.apply(devig_close, axis=1)\n",
        "        out[\"clv_prob\"] = out[\"p_close_over\"] - out[\"p_mkt_over\"]\n",
        "    else:\n",
        "        out[\"p_close_over\"] = np.nan\n",
        "        out[\"clv_prob\"] = np.nan\n",
        "\n",
        "    return out\n",
        "\n",
        "def walk_forward_backtest(odds_feat: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = odds_feat.sort_values(\"date\").reset_index(drop=True)\n",
        "    start_date = df[\"date\"].min()\n",
        "    end_date = df[\"date\"].max()\n",
        "    cut = start_date + pd.Timedelta(days=365)  # require at least 1 year warmup by default\n",
        "    preds_all = []\n",
        "\n",
        "    while cut < end_date:\n",
        "        train = feat_df[feat_df[\"date\"] < cut].copy()\n",
        "        test = df[(df[\"date\"] >= cut) & (df[\"date\"] < cut + pd.Timedelta(days=CFG.walk_forward_step_days))].copy()\n",
        "\n",
        "        if len(train) < CFG.min_train_rows or len(test) == 0:\n",
        "            cut += pd.Timedelta(days=CFG.walk_forward_step_days)\n",
        "            continue\n",
        "\n",
        "        # Fit models\n",
        "        minutes_model = fit_minutes_model(train)\n",
        "        shots_model = fit_nb_count_model(train.dropna(subset=SHOTS_FEATURES_NUM), \"shots\", SHOTS_FEATURES_NUM, SHOTS_FEATURES_CAT)\n",
        "        sot_direct_model = fit_nb_count_model(train.dropna(subset=SOT_FEATURES_NUM), \"sot\", SOT_FEATURES_NUM, SOT_FEATURES_CAT)\n",
        "        acc_model = fit_accuracy_model(train)\n",
        "\n",
        "        # Predict\n",
        "        block_preds = build_prediction_block(\n",
        "            test, minutes_model, shots_model, sot_direct_model, acc_model,\n",
        "            n_mc=min(CFG.count_mc_samples, 2000),  # keep fold runtime reasonable\n",
        "            rng=RNG\n",
        "        )\n",
        "        preds_all.append(block_preds)\n",
        "        print(f\"Fold @ {cut.date()} | train={len(train):,} test={len(test):,}\")\n",
        "\n",
        "        cut += pd.Timedelta(days=CFG.walk_forward_step_days)\n",
        "\n",
        "    if not preds_all:\n",
        "        raise RuntimeError(\"No folds produced. Reduce CFG.min_train_rows or adjust dates.\")\n",
        "    return pd.concat(preds_all, ignore_index=True)\n",
        "\n",
        "bt_preds = walk_forward_backtest(odds_feat)\n",
        "bt_preds.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20519e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 13) Backtest evaluation: calibration, Brier/logloss, and SOT model selection\n",
        "\n",
        "def eval_block(df: pd.DataFrame, p_col: str, label_col: str = \"y_over\") -> Dict[str, float]:\n",
        "    d = df.dropna(subset=[p_col, label_col]).copy()\n",
        "    if d.empty:\n",
        "        return {\"brier\": np.nan, \"logloss\": np.nan, \"n\": 0}\n",
        "    p = np.clip(d[p_col].to_numpy(), 1e-6, 1 - 1e-6)\n",
        "    y = d[label_col].astype(int).to_numpy()\n",
        "    return {\n",
        "        \"brier\": float(brier_score_loss(y, p)),\n",
        "        \"logloss\": float(log_loss(y, p)),\n",
        "        \"n\": int(len(d))\n",
        "    }\n",
        "\n",
        "def reliability_curve(y: np.ndarray, p: np.ndarray, bins: int = 10) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    p = np.clip(p, 1e-9, 1 - 1e-9)\n",
        "    edges = np.linspace(0, 1, bins + 1)\n",
        "    idx = np.digitize(p, edges) - 1\n",
        "    bin_centers = 0.5 * (edges[:-1] + edges[1:])\n",
        "    obs = np.full(bins, np.nan)\n",
        "    cnt = np.zeros(bins, dtype=int)\n",
        "    for b in range(bins):\n",
        "        m = idx == b\n",
        "        cnt[b] = m.sum()\n",
        "        if cnt[b] > 0:\n",
        "            obs[b] = y[m].mean()\n",
        "    return bin_centers, obs, cnt\n",
        "\n",
        "def plot_reliability(df: pd.DataFrame, p_col: str, title: str):\n",
        "    d = df.dropna(subset=[p_col, \"y_over\"]).copy()\n",
        "    p = d[p_col].to_numpy()\n",
        "    y = d[\"y_over\"].astype(int).to_numpy()\n",
        "    x, obs, cnt = reliability_curve(y, p, bins=10)\n",
        "    plt.figure()\n",
        "    plt.plot([0,1],[0,1])\n",
        "    plt.plot(x, obs, marker=\"o\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Predicted probability (bin center)\")\n",
        "    plt.ylabel(\"Observed frequency\")\n",
        "    plt.ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate by market\n",
        "shots_bt = bt_preds[bt_preds[\"market\"] == \"shots\"].copy()\n",
        "sot_bt = bt_preds[bt_preds[\"market\"] == \"sot\"].copy()\n",
        "\n",
        "print(\"Shots model (direct):\", eval_block(shots_bt, \"p_model_over_direct\"))\n",
        "print(\"SOT direct:\", eval_block(sot_bt, \"p_model_over_direct\"))\n",
        "print(\"SOT structural:\", eval_block(sot_bt, \"p_model_over_struct\"))\n",
        "\n",
        "plot_reliability(shots_bt, \"p_model_over_direct\", \"Reliability — Shots (model)\")\n",
        "plot_reliability(sot_bt, \"p_model_over_direct\", \"Reliability — SOT (direct)\")\n",
        "plot_reliability(sot_bt, \"p_model_over_struct\", \"Reliability — SOT (structural)\")\n",
        "\n",
        "# Choose SOT model based on Brier then calibration\n",
        "sot_direct_brier = eval_block(sot_bt, \"p_model_over_direct\")[\"brier\"]\n",
        "sot_struct_brier = eval_block(sot_bt, \"p_model_over_struct\")[\"brier\"]\n",
        "SOT_CHOICE = \"struct\" if sot_struct_brier < sot_direct_brier else \"direct\"\n",
        "print(\"Selected SOT model:\", SOT_CHOICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eecc9dca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 14) Strategy simulation and CLV summaries (simple baseline)\n",
        "\n",
        "def fractional_kelly(p: float, odds: float, frac: float=0.25, cap: float=0.03) -> float:\n",
        "    b = odds - 1.0\n",
        "    q = 1 - p\n",
        "    if b <= 0:\n",
        "        return 0.0\n",
        "    f = (b*p - q) / b\n",
        "    f = max(0.0, f) * frac\n",
        "    return float(min(f, cap))\n",
        "\n",
        "def apply_bet_filters(df: pd.DataFrame, p_col: str, ev_col: str) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out[\"minutes_ci_width\"] = out[\"min_ci_width\"]\n",
        "    out[\"pass_filters\"] = (\n",
        "        (out[\"p_start\"] >= CFG.min_p_start) &\n",
        "        (out[\"minutes_ci_width\"] <= CFG.max_minutes_ci_width) &\n",
        "        (out[ev_col] >= CFG.min_ev_mean)\n",
        "    )\n",
        "    return out\n",
        "\n",
        "def simulate_strategy(df: pd.DataFrame, p_col: str, ev_col: str, label_col: str=\"y_over\") -> pd.DataFrame:\n",
        "    d = apply_bet_filters(df, p_col, ev_col).copy()\n",
        "    d = d[d[\"pass_filters\"]].copy()\n",
        "    if d.empty:\n",
        "        return d\n",
        "    d[\"stake\"] = d.apply(lambda r: fractional_kelly(float(r[p_col]), float(r[\"odds_over\"])), axis=1)\n",
        "    d[\"pnl\"] = d[\"stake\"] * np.where(d[label_col].astype(int) == 1, d[\"odds_over\"] - 1.0, -1.0)\n",
        "    return d\n",
        "\n",
        "# Use chosen SOT model\n",
        "bt_preds[\"p_used\"] = np.where(\n",
        "    bt_preds[\"market\"] == \"shots\",\n",
        "    bt_preds[\"p_model_over_direct\"],\n",
        "    np.where(SOT_CHOICE == \"struct\", bt_preds[\"p_model_over_struct\"], bt_preds[\"p_model_over_direct\"])\n",
        ")\n",
        "bt_preds[\"ev_used\"] = np.where(\n",
        "    bt_preds[\"market\"] == \"shots\",\n",
        "    bt_preds[\"ev_over_direct\"],\n",
        "    np.where(SOT_CHOICE == \"struct\", bt_preds[\"ev_over_struct\"], bt_preds[\"ev_over_direct\"])\n",
        ")\n",
        "\n",
        "bets = simulate_strategy(bt_preds, \"p_used\", \"ev_used\")\n",
        "print(\"Bets:\", len(bets), \"Total PnL:\", bets[\"pnl\"].sum(), \"Avg stake:\", bets[\"stake\"].mean())\n",
        "\n",
        "# CLV summary if available\n",
        "if bets[\"clv_prob\"].notna().any():\n",
        "    print(\"Mean CLV (prob):\", bets[\"clv_prob\"].mean())\n",
        "    print(\"Share positive CLV:\", (bets[\"clv_prob\"] > 0).mean())\n",
        "\n",
        "# Plot cumulative pnl\n",
        "if len(bets) > 0:\n",
        "    b = bets.sort_values(\"date\").copy()\n",
        "    b[\"cum_pnl\"] = b[\"pnl\"].cumsum()\n",
        "    plt.figure()\n",
        "    plt.plot(b[\"date\"], b[\"cum_pnl\"])\n",
        "    plt.title(\"Cumulative PnL (baseline strategy)\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"PnL (units)\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a807af39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 15) Candidate generation for upcoming matchweek (using upcoming_odds.csv)\n",
        "\n",
        "def make_candidates(\n",
        "    upcoming_odds: pd.DataFrame,\n",
        "    feat_df: pd.DataFrame,\n",
        "    minutes_model: MinutesModel,\n",
        "    shots_model: NBModel,\n",
        "    sot_direct_model: NBModel,\n",
        "    acc_model: BinomModel,\n",
        "    n_mc: int,\n",
        "    rng: np.random.Generator,\n",
        "    sot_choice: str\n",
        ") -> pd.DataFrame:\n",
        "    # Join features for the player-match rows. You must have those rows present pre-match.\n",
        "    # Practical approach: build a placeholder row for each upcoming odds line using latest known features.\n",
        "    # Here we approximate by taking each player's most recent feature row and overwriting match context if you have it.\n",
        "    last_feat = (\n",
        "        feat_df.sort_values(\"date\")\n",
        "              .groupby(\"player_id\", as_index=False)\n",
        "              .tail(1)\n",
        "    )\n",
        "    base = upcoming_odds.merge(last_feat, on=\"player_id\", how=\"left\", suffixes=(\"\", \"_last\"))\n",
        "    # Set date and match_id from odds\n",
        "    base[\"date\"] = parse_date(base[\"date\"])\n",
        "    # If you have team/opponent/home_away for upcoming, include them in upcoming_odds and overwrite here.\n",
        "    # Otherwise we keep last known team/opponent as a rough approximation (you should improve this for real use).\n",
        "\n",
        "    preds = build_prediction_block(\n",
        "        base, minutes_model, shots_model, sot_direct_model, acc_model,\n",
        "        n_mc=n_mc, rng=rng\n",
        "    )\n",
        "\n",
        "    preds[\"p_used\"] = np.where(\n",
        "        preds[\"market\"] == \"shots\",\n",
        "        preds[\"p_model_over_direct\"],\n",
        "        np.where(sot_choice == \"struct\", preds[\"p_model_over_struct\"], preds[\"p_model_over_direct\"])\n",
        "    )\n",
        "    preds[\"ev_used\"] = preds.apply(lambda r: ev_decimal(float(r[\"p_used\"]), float(r[\"odds_over\"])), axis=1)\n",
        "\n",
        "    # Conservative lower bound via a quick MC re-run with fewer draws + using p10 minutes (simple proxy)\n",
        "    preds[\"p_conservative\"] = np.clip(preds[\"p_used\"] - 0.03, 0.0, 1.0)  # replace with bootstrap if desired\n",
        "    preds[\"ev_low\"] = preds.apply(lambda r: ev_decimal(float(r[\"p_conservative\"]), float(r[\"odds_over\"])), axis=1)\n",
        "\n",
        "    preds[\"stake_rec\"] = preds.apply(lambda r: fractional_kelly(float(r[\"p_used\"]), float(r[\"odds_over\"])), axis=1)\n",
        "\n",
        "    preds[\"risk_flag_rotation\"] = (preds[\"p_start\"] < CFG.min_p_start).astype(int)\n",
        "    preds[\"risk_flag_minutes_wide\"] = (preds[\"min_ci_width\"] > CFG.max_minutes_ci_width).astype(int)\n",
        "\n",
        "    # Filters\n",
        "    preds[\"pass_filters\"] = (\n",
        "        (preds[\"ev_used\"] >= CFG.min_ev_mean) &\n",
        "        ((preds[\"ev_low\"] > 0) if CFG.require_ev_low_positive else True) &\n",
        "        (preds[\"p_start\"] >= CFG.min_p_start) &\n",
        "        (preds[\"min_ci_width\"] <= CFG.max_minutes_ci_width)\n",
        "    )\n",
        "\n",
        "    cols = [\n",
        "        \"date\",\"match_id\",\"player_id\",\"player_name\",\"team\",\"opponent\",\"market\",\"line\",\n",
        "        \"book\",\"timestamp\",\"odds_over\",\"odds_under\",\n",
        "        \"p_mkt_over\",\"p_used\",\"p_conservative\",\n",
        "        \"ev_used\",\"ev_low\",\"stake_rec\",\n",
        "        \"p_start\",\"min_mean\",\"min_p10\",\"min_p90\",\"min_ci_width\",\n",
        "        \"risk_flag_rotation\",\"risk_flag_minutes_wide\",\n",
        "        \"pass_filters\"\n",
        "    ]\n",
        "    for c in cols:\n",
        "        if c not in preds.columns:\n",
        "            preds[c] = np.nan\n",
        "    preds = preds[cols].sort_values([\"pass_filters\",\"ev_used\"], ascending=[False, False])\n",
        "    return preds\n",
        "\n",
        "if os.path.exists(CFG.upcoming_odds_path):\n",
        "    upcoming_odds = load_odds_df(CFG.upcoming_odds_path)\n",
        "    candidates = make_candidates(\n",
        "        upcoming_odds, feat_df,\n",
        "        minutes_model_all, shots_nb_all, sot_nb_all, acc_model_all,\n",
        "        n_mc=CFG.count_mc_samples, rng=RNG,\n",
        "        sot_choice=SOT_CHOICE\n",
        "    )\n",
        "    display(candidates.head(30))\n",
        "else:\n",
        "    print(\"No upcoming_odds.csv found at\", CFG.upcoming_odds_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09bb28df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 16) Exports (candidates + model_version)\n",
        "\n",
        "def export_outputs(candidates: Optional[pd.DataFrame], out_dir: str = \"outputs\") -> None:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    version = {\n",
        "        \"run_datetime\": str(pd.Timestamp.now()),\n",
        "        \"devig_method\": CFG.devig_method,\n",
        "        \"sot_choice\": SOT_CHOICE,\n",
        "        \"config\": asdict(CFG),\n",
        "        \"note\": \"This notebook does not guarantee profit; validate with calibration + CLV over time.\"\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"model_version.json\"), \"w\") as f:\n",
        "        json.dump(version, f, indent=2)\n",
        "\n",
        "    if candidates is not None:\n",
        "        candidates.to_csv(os.path.join(out_dir, \"candidates.csv\"), index=False)\n",
        "        print(\"Wrote:\", os.path.join(out_dir, \"candidates.csv\"))\n",
        "    print(\"Wrote:\", os.path.join(out_dir, \"model_version.json\"))\n",
        "\n",
        "if \"candidates\" in locals():\n",
        "    export_outputs(candidates)\n",
        "else:\n",
        "    export_outputs(None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f9708b",
      "metadata": {},
      "source": [
        "# Assumptions & limitations\n",
        "\n",
        "- **Data provenance & terms:** This notebook’s default “free-ish” stats source is **FBref via `soccerdata`** (cached locally). You are responsible for complying with the data providers’ terms, robots rules, and rate limits. If you need guaranteed stability/compliance, use a licensed provider (Opta/StatsPerform, Wyscout, etc.).\n",
        "- **SOT definition:** We infer “shots on target” from FBref shot-event outcomes containing **Goal** or **Saved**. If you have an official SOT field, swap it in (and re-run the backtest).\n",
        "- **Odds history is the bottleneck:** A real edge test needs *historical* offered prices (and ideally a “closing” snapshot for CLV). If you only have current odds, you can still validate calibration of the model, but you can’t credibly validate “beating the market.”\n",
        "- **Historical player props availability:** The Odds API’s historical coverage for non-featured markets (like player props) starts later than the underlying match stats. For earlier seasons you’ll need (a) your own archived snapshots, or (b) a paid/vendor feed.\n",
        "- **Name matching is imperfect:** Odds feeds usually key props by **player name**, not a stable ID. This notebook uses a canonicalized-name hash as `player_id`, which can collide for rare duplicates and can break on diacritics / name changes / mid-season transfers. Always spot-check candidates.\n",
        "- **Minutes uncertainty dominates:** If your minutes model is wrong (rotation, injuries, lineup shocks), shots/SOT prop pricing will be wrong. Treat the minutes model as a first-class component.\n",
        "- **Market efficiency:** Shots/SOT props for popular players can be very efficient. “No stable edge detected” is a valid result.\n",
        "\n",
        "Next improvements (high ROI):\n",
        "- Add an external projected-lineups signal to the minutes model (even a weak one helps).\n",
        "- Improve upcoming-match context features (opponent allowance, home/away) instead of copying the last match’s features.\n",
        "- Replace the conservative EV proxy with a proper bootstrap/posterior interval on `p_over`.\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}